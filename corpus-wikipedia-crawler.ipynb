{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text_wiki (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/nusretipek/LanguageFinder\n",
    "\"\"\"\n",
    "Wikipedia corpus builder is designed to harvest random Wikipedia pages in a given \n",
    "language code such as es for Spanish and train the ngram weights based on these pages. \n",
    "The full list of the available languages can be found in https://en.wikipedia.org/wiki/List_of_Wikipedias\n",
    "## Example\n",
    "```julia \n",
    "julia> train_wikipedia_text(\"es\", 10, 15)\n",
    "julia> \n",
    "    \"Successfully trained on 10 es wikipedia pages.\"\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "using HTTP\n",
    "\n",
    "\n",
    "#=\n",
    "The dictionaries is stored within text files with custom format.\n",
    "This function reads the custom text format and build a dictionary variable.\n",
    "Retruns the dictionary read from the text file.\n",
    "=#\n",
    "\n",
    "function read_dictionary(InputFile::String)\n",
    "    f = open(InputFile)\n",
    "    raw_text = readlines(f)\n",
    "    close(f)    \n",
    "    dictionary = Dict()\n",
    "    for i in raw_text\n",
    "        push!(dictionary,split(i)[1] => split(i)[2]) end\n",
    "    return dictionary\n",
    "end\n",
    "\n",
    "#=\n",
    "The dictionaries are written with specific format for later use.\n",
    "Specifically each line ad key + \" \" + value. This function writes\n",
    "a defined dictionary to a text file.\n",
    "Retruns nothing.\n",
    "=#\n",
    "\n",
    "function write_dictionary(InputFile::String, DICT::Dict)\n",
    "    f = open(InputFile, \"w\")\n",
    "    for (key, value) in DICT\n",
    "        println(f, key, \" \", value) end\n",
    "    close(f)\n",
    "end\n",
    "\n",
    "#=\n",
    "The Wikipedia page request function. Each Wikipedia subdomain has different random page url, instead of \n",
    "requesting that url, there is a known random page url text file stored and automatically updated when\n",
    "a new language WP code is used. (\"Wikipedia_Random.txt\") \n",
    "HTTP library is used to get the webpage and the body text is pipelined to string.\n",
    "Retruns the String type webpage body text.\n",
    "=#\n",
    "\n",
    "function get_random_wikipedia_page(LANG_CODE::String)\n",
    "    url = \"\"\n",
    "    random_urls = read_dictionary(\"Wikipedia_Random.txt\")\n",
    "    if(haskey(random_urls, LANG_CODE))\n",
    "        url = \"https://\" * LANG_CODE * \".wikipedia.org\" * random_urls[LANG_CODE]\n",
    "    else\n",
    "        homepage = HTTP.get(\"https://\" * LANG_CODE * \".wikipedia.org\" * \"/wiki/\")\n",
    "        str = homepage.body |> String\n",
    "        start_point = findfirst(\"n-randompage\", str).stop+3\n",
    "        text_rest = str[start_point:end]\n",
    "        random_href = SubString(text_rest, findall(\"\\\"\", text_rest)[1].start+1, findall(\"\\\"\", text_rest)[2].start-1)\n",
    "        push!(random_urls, LANG_CODE => random_href)\n",
    "        write_dictionary(\"Wikipedia_Random.txt\", random_urls)  \n",
    "        url = \"https://\" * LANG_CODE * \".wikipedia.org\" * random_urls[LANG_CODE] end\n",
    "    r = HTTP.get(url)\n",
    "    title = r.request.target\n",
    "\ttitle = replace(title, r\"/\" => \"_\")\n",
    "\ttitle = replace(title, r\":\" => \"_\")\n",
    "    return title, r.body |> String\n",
    "end\n",
    "\n",
    "#=\n",
    "The raw HTML string is not parsed as a tree and with a lot of standartized HTML tags.\n",
    "It is necessary to take the useful string in between these tags. The extract element function\n",
    "checks crawls in the HTML raw string and extract strings.\n",
    "Retruns an Array of useful strings.\n",
    "=#\n",
    "\n",
    "function extract_elements(HTML::AbstractString, ELEMENT::String)\n",
    "    open_p = findall(\"<\"*ELEMENT, HTML)\n",
    "    close_p = findall(\"</\"*ELEMENT, HTML)\n",
    "    arr = []\n",
    "    try\n",
    "        for i in 1:length(open_p)\n",
    "            temp_text = SubString(HTML, open_p[i].start, close_p[i].stop+1) \n",
    "            push!(arr, temp_text) end catch x end\n",
    "    return arr\n",
    "end\n",
    "\n",
    "#=\n",
    "The strings removed from the HTML tags are still contaminated with the inline annotations with \n",
    "(),[],{} and etc. The text or other information inside is often not useful to train ngrams. \n",
    "This function cleans these special set charaters and the information inside.\n",
    "Retruns a cleared string.\n",
    "=#\n",
    "\n",
    "function clean_inside_tags(TEXT::AbstractString, SYMBOL_START::String, SYMBOL_STOP::String)\n",
    "    open_symbol = findall(SYMBOL_START, TEXT)\n",
    "    close_symbol = findall(SYMBOL_STOP, TEXT)\n",
    "    arr = []\n",
    "    if(length(open_symbol) > 0 && length(close_symbol) > 0 && length(open_symbol) == length(close_symbol))\n",
    "        for i in 1:min(length(open_symbol), length(close_symbol))   \n",
    "            temp_text = SubString(TEXT, open_symbol[i].start, close_symbol[i].stop)\n",
    "            push!(arr, temp_text) end end\n",
    "    for j in arr\n",
    "        TEXT = replace(TEXT, j => \"\") end\n",
    "    return TEXT\n",
    "end\n",
    "\n",
    "#=\n",
    "This function combines the extracting and cleaning most prominent sets from a random Wikipedia\n",
    "page. It extract the text within the <p> tags (Always the case in Wikipedia). Then, utilize the \n",
    "clean_inside_tags function to clear <>, (), [] and {}.\n",
    "Returns a String with clean text.\n",
    "=#\n",
    "\n",
    "function clean_text_wiki(HTML::AbstractString)\n",
    "    temp_text = \"\"\n",
    "    for i in extract_elements(HTML, \"p\")\n",
    "        temp_text *= clean_inside_tags(clean_inside_tags(clean_inside_tags(clean_inside_tags(i, \"<\", \">\"), \"[\", \"]\"), \"(\", \")\"), \"{\", \"}\") end\n",
    "    return temp_text\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_random_wikipedia_page(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "download_wikipedia_text (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "using SHA\n",
    "\n",
    "function html2text(content::AbstractString)\n",
    "    patterns = [\n",
    "        r\"<[\\s]*?script[^>]*?>[\\s\\S]*?<[\\s]*?/[\\s]*?script[\\s]*?>\" => \" \",\n",
    "        r\"<[\\s]*?style[^>]*?>[\\s\\S]*?<[\\s]*?/[\\s]*?style[\\s]*?>\" => \" \",\n",
    "        r\"<!--[\\s\\S]*?-->\" => \" \",\n",
    "        \"<br>\" => \"\\n\",\n",
    "        r\"<[\\s\\S]*?>\" => \" \",\n",
    "        \"&nbsp;\" => \" \",\n",
    "        \"&quot;\" => \"\\\"\",\n",
    "        \"&amp;\" => \"&\",\n",
    "        \"&lt;\" => \"<\",\n",
    "        \"&gt;\" => \">\",\n",
    "        r\"&#?\\w{1,6};\" => \" \",\n",
    "    ]\n",
    "    for p in patterns\n",
    "        content = replace(content, p)\n",
    "    end\n",
    "    content\n",
    "end\n",
    "\n",
    "function download_wikipedia_text(lang_code::String, pages::Integer=1, sleep_time::Integer = 10; path=\"./corpus\")\n",
    "\tlang_code = lowercase(lang_code)\n",
    "    # try HTTP.get(\"https://\" * lang_code * \".wikipedia.org\" * \"/wiki/\") catch x throw(ArgumentError(\"Invalid language code; check WP codes in https://en.wikipedia.org/wiki/List_of_Wikipedias\")) end\n",
    "\tdirpath = joinpath(path, lang_code)\n",
    "\tmkpath(dirpath)\n",
    "    try\n",
    "        for i in length(readdir(dirpath))+1:pages\n",
    "            if ispath(joinpath(path, \"stop\"))\n",
    "                return\n",
    "            end\n",
    "            title, text = get_random_wikipedia_page(lang_code)\n",
    "            if length(title) > 47\n",
    "                title = title[1:20] * \"_\" * bytes2hex(sha1(title))[1:5] * \"_\" * title[end-19:end]\n",
    "            end\n",
    "            text = clean_text_wiki(text)\n",
    "            text = html2text(text)\n",
    "            text = replace(text, r\"\\n\\n+\" => \"\\n\")\n",
    "\t\t\tfn = joinpath(dirpath, title*\".txt\")\n",
    "            if ispath(fn)\n",
    "                print(\"!+ \")\n",
    "            end\n",
    "\t\t\tprint(i, \". \", abspath(fn))\n",
    "\t\t\tif ! isempty(text)\n",
    "\t            open(fn, \"w\") do f\n",
    "\t                write(f, text) \n",
    "\t            end\n",
    "\t\t\tend\n",
    "            println(\" âœ“\")\n",
    "            flush(stdout)\n",
    "            sleep(sleep_time)\n",
    "        end\n",
    "\tcatch e\n",
    "\t\tthrow(e)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_wikipedia_text(\"en\", path=raw\"./corpus/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(langs) = 25\n",
      "log_file = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"log_test_2023-08-10_18-19-09.txt\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "langs = [\"ar\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"es\", \"fa\", \"fi\", \"fr\", \"he\", \"hi\", \"hu\", \"it\", \"jp\", \"ko\", \"nl\", \"no\", \"pl\", \"pt\", \"ru\", \"sv\", \"tr\", \"uk\", \"zh\"]\n",
    "\n",
    "@show length(langs)\n",
    "step = 10\n",
    "total = 200\n",
    "path=raw\"./corpus/wikipedia/test\"\n",
    "using Dates\n",
    "\n",
    "time_str = Dates.format(Dates.now(), \"yyyy-mm-dd_HH-MM-SS\")\n",
    "log_file = \"log_\" * basename(path) * \"_$time_str.txt\"\n",
    "@show log_file\n",
    "redirect_stdio(stdout=log_file) do\n",
    "\tfor i in [step:step:total; total]\n",
    "\t\ttime_str = Dates.format(Dates.now(), \"yyyy-mm-dd_HH-MM-SS\")\n",
    "\t\tprintln(\"### $i ### [$time_str]\")\n",
    "\t\tfor lang in langs\n",
    "\t\t\tdownload_wikipedia_text(lang, i, path=path)\n",
    "\t\tend\n",
    "\tend\n",
    "end\n",
    "# touch corpus/wikipedia/test/stop\n",
    "# rm corpus/wikipedia/test/stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "langs = [\"ar\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"es\", \"fa\", \"fi\", \"fr\", \"he\", \"hi\", \"hu\", \"it\", \"jp\", \"ko\", \"nl\", \"no\", \"pl\", \"pt\", \"ru\", \"sv\", \"tr\", \"uk\", \"zh\"]\n",
    "\n",
    "@show length(langs)\n",
    "step = 50\n",
    "total = 5000\n",
    "path=raw\"./corpus/wikipedia/train\"\n",
    "using Dates\n",
    "\n",
    "time_str = Dates.format(Dates.now(), \"yyyy-mm-dd_HH-MM-SS\")\n",
    "log_file = \"log_\" * basename(path) * \"_$time_str.txt\"\n",
    "@show log_file\n",
    "redirect_stdio(stdout=log_file) do\n",
    "\tfor i in [step:step:total; total]\n",
    "\t\ttime_str = Dates.format(Dates.now(), \"yyyy-mm-dd_HH-MM-SS\")\n",
    "\t\tprintln(\"### $i ### [$time_str]\")\n",
    "\t\tfor lang in langs\n",
    "\t\t\tdownload_wikipedia_text(lang, i, path=path)\n",
    "\t\tend\n",
    "\tend\n",
    "end\n",
    "# touch corpus/wikipedia/train/stop\n",
    "# rm corpus/wikipedia/train/stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed _wiki_Artemis_Fowl:_Evighedskoden.txt to _wiki_Artemis_Fowl__Evighedskoden.txt\n",
      "Renamed _wiki_B%C3%A5nd_1:_Ernst.txt to _wiki_B%C3%A5nd_1__Ernst.txt\n",
      "Renamed _wiki_Rush_ABC:_Live_d8cb9__Agora_Ballroom_1974.txt to _wiki_Rush_ABC__Live_d8cb9__Agora_Ballroom_1974.txt\n",
      "Renamed _wiki_Tales_of_Aravorn:_Seasons_of_the_Wolf.txt to _wiki_Tales_of_Aravorn__Seasons_of_the_Wolf.txt\n",
      "Renamed _wiki_Anexo:Abierto__62288_ndividual_masculino).txt to _wiki_Anexo_Abierto__62288_ndividual_masculino).txt\n",
      "Renamed _wiki_Anexo:Medaller_d6ae9_tud_de_Singapur_2010.txt to _wiki_Anexo_Medaller_d6ae9_tud_de_Singapur_2010.txt\n",
      "Renamed _wiki_Anexo:Personajes_de_Read_or_Die.txt to _wiki_Anexo_Personajes_de_Read_or_Die.txt\n",
      "Renamed _wiki_Anexo:Plantill_04bd9_tem_Pro_Cycling_Team.txt to _wiki_Anexo_Plantill_04bd9_tem_Pro_Cycling_Team.txt\n",
      "Renamed _wiki_Anexo:Promoci%_4473e__de_Espa%C3%B1a_2008.txt to _wiki_Anexo_Promoci%_4473e__de_Espa%C3%B1a_2008.txt\n",
      "Renamed _wiki_Anexo:Sismos_en_Chile_de_2014.txt to _wiki_Anexo_Sismos_en_Chile_de_2014.txt\n",
      "Renamed _wiki_Anexo:Tiro_en__9d2cc_icos_de_Londres_2012.txt to _wiki_Anexo_Tiro_en__9d2cc_icos_de_Londres_2012.txt\n",
      "Renamed _wiki_Anexo:Torneo_d_41139_3_(dobles_masculino).txt to _wiki_Anexo_Torneo_d_41139_3_(dobles_masculino).txt\n",
      "Renamed _wiki_God:_The_Failed_Hypothesis.txt to _wiki_God__The_Failed_Hypothesis.txt\n",
      "Renamed _wiki_Mario_Kart_Live:_Home_Circuit.txt to _wiki_Mario_Kart_Live__Home_Circuit.txt\n",
      "Renamed _wiki_My_Bebe_Love:_KiligPaMore.txt to _wiki_My_Bebe_Love__KiligPaMore.txt\n",
      "Renamed _wiki_Last_Night_on_Earth:_Live_in_Tokyo.txt to _wiki_Last_Night_on_Earth__Live_in_Tokyo.txt\n",
      "Renamed _wiki_Vampires_3_:_L_9cecd_3%89clipse_du_soleil.txt to _wiki_Vampires_3___L_9cecd_3%89clipse_du_soleil.txt\n",
      "Renamed _wiki_%E0%A4%AF%E0%A_bb0ec_E:%E0%A5%A9%E0%A5%A6.txt to _wiki_%E0%A4%AF%E0%A_bb0ec_E_%E0%A5%A9%E0%A5%A6.txt\n",
      "Renamed _wiki_Superman_II.:__3558c_Donner-v%C3%A1ltozat.txt to _wiki_Superman_II.___3558c_Donner-v%C3%A1ltozat.txt\n",
      "Renamed _wiki_Ricreazione:_Natale_sulla_terza_strada.txt to _wiki_Ricreazione__Natale_sulla_terza_strada.txt\n",
      "Renamed _wiki_%EB%A1%A4%ED%9_e528b_:_%EB%8F%99%EC%B9%A8.txt to _wiki_%EB%A1%A4%ED%9_e528b___%EB%8F%99%EC%B9%A8.txt\n",
      "Renamed _wiki_Unlock_:_GO_LIVE_IN_LIFE.txt to _wiki_Unlock___GO_LIVE_IN_LIFE.txt\n",
      "Renamed _wiki_Lijst_van_teru_8aa6a_Star_Trek:_Discovery.txt to _wiki_Lijst_van_teru_8aa6a_Star_Trek__Discovery.txt\n",
      "Renamed _wiki_Boginja:_kak_ja_poljubila.txt to _wiki_Boginja__kak_ja_poljubila.txt\n",
      "Renamed _wiki_%C3%94-Genio:_Live_in_Brazil,_1963.txt to _wiki_%C3%94-Genio__Live_in_Brazil,_1963.txt\n",
      "Renamed _wiki_Miracles:_The_Holiday_Album.txt to _wiki_Miracles__The_Holiday_Album.txt\n",
      "Renamed _wiki_Tom_Clancy%27s_Ghost_Recon:_Desert_Siege.txt to _wiki_Tom_Clancy%27s_Ghost_Recon__Desert_Siege.txt\n",
      "Renamed _wiki_4:e_divisionen_(Tyskland).txt to _wiki_4_e_divisionen_(Tyskland).txt\n",
      "Renamed _wiki_Avanak_Ajan:_Cazibesi_Yeter.txt to _wiki_Avanak_Ajan__Cazibesi_Yeter.txt\n",
      "Renamed _wiki_Ben_10:_10.010.txt to _wiki_Ben_10__10.010.txt\n",
      "Renamed _wiki_Hell:_The_Sequel.txt to _wiki_Hell__The_Sequel.txt\n",
      "Renamed _wiki_UTC%2B06:00.txt to _wiki_UTC%2B06_00.txt\n"
     ]
    }
   ],
   "source": [
    "for (root, dirs, files) in walkdir(raw\"./corpus/wikipedia/train\")\n",
    "    for file in files\n",
    "        if occursin(\":\", file)\n",
    "            newname = replace(file, \":\" => \"_\")\n",
    "            mv(joinpath(root, file), joinpath(root, newname))\n",
    "            println(\"Renamed $file to $newname\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
