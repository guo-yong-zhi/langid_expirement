У теорії ймовірностей, моделі Маркова це стохастичні моделі, які використовуються для моделювання систем, що випадково змінюються, де передбачається, що майбутні стани залежать тільки від поточного стану, а не від послідовності подій, які передували цьому . Як правило, це припущення дозволяє міркування і обчислення з моделлю, яка б в іншому випадку лишилась нерозв'язною.
Є чотири загальних моделей Маркова, використовувані в різних ситуаціях, залежно від того, кожен послідовний стан спостерігається чи ні, і чи буде система скорегована на підставі спостережень, зроблених:
Найпростіша модель Маркова це ланцюг Маркова. Він моделює стан системи з випадковою змінною, що змінюється в часі. У цьому контексті, Марков передбачає, що розподіл цієї змінної залежить тільки від розподілу в попередній стан. Приклад використання ланцюга Маркова — ланцюг Маркова Монте-Карло, який використовує властивість Маркова, щоб довести, що конкретний метод для виконання випадкового блукання буде зразком зі спільного розподілу системи.
Прихована модель Маркова це ланцюг Маркова, для якого стан лише частково спостерігається. Іншими словами, спостереження, пов'язані зі станом системи, але їх, як правило, недостатньо, щоб точно визначити стан. Існує кілька відомих алгоритмів для прихованих моделей Маркова. Наприклад, враховуючи послідовність спостережень, алгоритм Вітербо обчислює найбільш ймовірну відповідну послідовність станів, далі алгоритм обчислення ймовірності послідовності спостережень, і алгоритм Баума-Уелча буде оцінювати стартові ймовірності, перехід функції, та функцію спостереження прихованої моделі Маркова.
Одне з поширених використань — для розпізнавання мови, де спостережувані дані є мова аудіо сигналу і приховані стани є проголошенням тексту. У цьому прикладі, алгоритм Вітербо знаходить найбільш ймовірну послідовність вимовлених слів даних мовних звуків.
Марковський процес ухвалення рішень це  ланцюг Маркова, в якому станові переходи залежать від поточного стану та вектора дій, який застосовується до системи. Як правило, марковський процес ухвалення рішень використовується для обчислення політики дій, які будуть максимізувати деякі утиліти по відношенню до очікуваних винагородою. Це тісно пов'язано з підкріпленням, і може бути вирішено із значенням ітерації і суміжних методів.
Частково спостережуваний марковський процес ухвалення рішень  є марковський процес ухвалення рішень, в якому стан системи тільки частково спостерігається. Недавні методи апроксимації зробили їх корисними для різних застосувань, таких як контроль простих агентів або роботів 1 .
Випадкове поле Маркова, або мережа Маркова, може вважатися узагальненням ланцюга Маркова в декількох вимірах. У ланцюзі Маркова, стан залежить тільки від попереднього стану в момент, в той час як в марковському випадковому полі, кожен стан залежить від своїх сусідів у будь-якому з декількох напрямів. Марківське випадкове поле може бути візуалізоване в області графіка або випадкових величин, де розподіл кожної випадкової змінної залежить від сусідніх змінних, з якою він з'єднаний. Більш конкретно, спільний розподіл для будь-якої випадкової змінної в графі може бути обчислений як добуток «кліки потенціалів» всіх кліків в графі, які містять цю випадкову змінну. Моделювання проблеми у вигляді марківського випадкового поля є корисним, тому що це означає, що спільні розподілу в кожної вершини в графі можуть бути обчислені в цій манері.
  Портал «Математика»                        
